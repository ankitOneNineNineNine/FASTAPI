# -*- coding: utf-8 -*-
"""SMS spam detection.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/19cq5PuKNDtpuhh6ai3OV3_VY_2vDRGwy
"""

import pandas as pd
import numpy as np
import re
import collections
import seaborn as sns
import matplotlib.pyplot as plt
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import warnings
warnings.simplefilter(action='ignore', category=Warning)

# !pip install contractions

#import contractions

import nltk
#nltk.download('wordnet')

#nltk.download('stopwords')

# from google.colab import files
# uploads = files.upload()

# from google.colab import drive
# drive.mount('/content/drive')

df = pd.read_csv('spam.csv', encoding='latin-1')
#df.head(5)

df.drop(["Unnamed: 2", "Unnamed: 3", "Unnamed: 4"], axis=1, inplace=True)

df.columns = ["SpamHam","Message"]

##sns.countplot(df["SpamHam"])

#Preprocessing started with of finding words along with count using counter
def word_count_visualize(data):
     word_counter = collections.Counter([word for sentence in data for word in sentence.split()])
     most_count = word_counter.most_common(30) #most frequent 30 words
     most_count = pd.DataFrame(most_count, columns=["Word", "Count"]).sort_values(by="Count")
     most_count.plot.barh(x = "Word", y = "Count", color="lightblue", figsize=(10, 15))

word_count_visualize(df["Message"])

#we observe that most of the words are stop words so we remove them
lem = WordNetLemmatizer()
def preprocessing(data):
      #sms = contractions.fix(data) # converting shortened words to original (Eg:"I'm" to "I am")
      sms = data
      sms = sms.lower() # lower casing the sms
      sms = re.sub(r'https?://S+|www.S+', "", sms).strip() #removing url
      sms = re.sub("[^a-z ]", "", sms) # removing symbols and numbes
      sms = sms.split() #splitting
      # lemmatization and stopword removal
      sms = [lem.lemmatize(word) for word in sms if not word in set(stopwords.words("english"))]
      sms = " ".join(sms)
      return sms

X = df["Message"].apply(preprocessing)

word_count_visualize(X)

from sklearn.feature_extraction.text import CountVectorizer

import string

def text_processing(mess):
    nopunc = [char for char in mess if char not in string.punctuation]
    
    nopunc = ''.join(nopunc)
    
    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]

df['Message'].head().apply(text_processing)

bow_transformer = CountVectorizer(analyzer=text_processing).fit(df['Message'])

#bow_transformer.vocabulary_

#print(len(bow_transformer.vocabulary_))

msg = df['Message'][3]

bow4 = bow_transformer.transform([msg])

#bow4.shape

#print(bow4)

messages_bow =bow_transformer.transform(df['Message'])

#messages_bow.shape

#messages_bow.nnz

sparsity = 100.0*messages_bow.nnz/(messages_bow.shape[0]*messages_bow.shape[1])
#sparsity

from sklearn.feature_extraction.text import TfidfTransformer

tfidf_transformer = TfidfTransformer().fit(messages_bow)

tfidf4 = tfidf_transformer.transform(bow4)
#print(tfidf4)

tfidf_transformer.idf_[bow_transformer.vocabulary_['free']]

messages_tfidf = tfidf_transformer.transform(messages_bow)

#messages_tfidf.shape

from sklearn.naive_bayes import MultinomialNB

spam_detect_model = MultinomialNB().fit(messages_tfidf, df['Message'])

spam_detect_model.predict(tfidf4)[0]

all_pred = spam_detect_model.predict(messages_tfidf)

#all_pred

from sklearn.model_selection import train_test_split

msg_train,msg_test,label_train,label_test = train_test_split(df['Message'],
                                                            df['SpamHam'], 
                                                            test_size = 0.3)

from sklearn.pipeline import Pipeline

# from sklearn.ensemble import RandomForestClassifier

pipeline = Pipeline([
    ('bow', CountVectorizer(analyzer=text_processing)),
    ('tfidf', TfidfTransformer()),
    ('classifier', MultinomialNB())
])

pipeline.fit(msg_train, label_train)

predictions = pipeline.predict(msg_test)

# from sklearn.metrics import classification_report,confusion_matrix

# print(classification_report(label_test, predictions))

# print(confusion_matrix(label_test, predictions))



"""Random Forest Classifier"""

# from sklearn.ensemble import RandomForestClassifier

# pipeline2 = Pipeline([
#     ('bow', CountVectorizer(analyzer=text_processing)),
#     ('tfidf', TfidfTransformer()),
#     ('classifier', RandomForestClassifier())
# ])

# pipeline2.fit(msg_train, label_train)

# predictions2 = pipeline2.predict(msg_test)
# print(classification_report(label_test, predictions2))

# print(confusion_matrix(label_test, predictions2))

# predict_msg =["You are awarded a Nikon Digital Camera. Call now","Call me","What's up?","Congratulations!! won a iPhone 13 Pro Max. Call now."]

# pipeline.predict(predict_msg)

# pipeline2.predict(predict_msg)

# import pickle

# Pkl_Filename = "Spam_detection_NB.pkl" 

# with open(Pkl_Filename, 'wb') as file:  
#     pickle.dump(pipeline, file)

# with open(Pkl_Filename, 'rb') as file:  
#     Pickled_Spam_detection_using_NB_Model = pickle.load(file)

# Pickled_Spam_detection_using_NB_Model.predict(predict_msg)

# import joblib

# joblib_file = "Spam_detection_RF_joblib.pkl"  
# joblib.dump(pipeline2, joblib_file)